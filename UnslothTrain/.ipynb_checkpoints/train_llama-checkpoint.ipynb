{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb43b78c-6717-494c-86dc-c1d16e30905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69530d8-9ffd-4d98-bc93-a8c740f9ca90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16e2cc-e954-438e-8c6c-17ef4ddad467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 6000 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1455c54-50f5-4a3a-9133-8257556392d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc619bf8-04fb-461f-85fa-de3188a82715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd56f5-07f5-4824-a4b2-ef0ba31b89a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d34e705-58ec-4c20-bda1-f6f53bf26164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('../Dataset/labels.json', 'r') as json_file:\n",
    "    narratives = json.load(json_file)\n",
    "    \n",
    "#train data\n",
    "with open('../Dataset/combined_EN_HI_PT_BG_RU.json', 'r') as json_file: # download data from task website\n",
    "    train_df_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56717f73-64d4-4dd8-a611-2d16e4894e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_dict[0] # see format of data\n",
    "\"\"\"\n",
    "{'file': 'EN_CC_100013.txt',\n",
    " 'narrative': 'CC: Criticism of climate movement',\n",
    " 'sub_narrative': 'CC: Criticism of climate movement: Ad hominem attacks on key activists',\n",
    " 'narratives_list': ['Criticism of climate movement'],\n",
    " 'subnarratives_list': ['Ad hominem attacks on key activists'],\n",
    " 'category': 'CC',\n",
    " 'file_Content': 'Bill Gates Says He Is ‘The Solution’ To Climate Change So It’s OK To Own Four Private Jets \\n\\nBill Gates has the right to fly around the world on private jets while normal people are forced to live in 15 minute cities without freedom of travel, according to Bill Gates himself, who told the BBC he is doing much more than anybody else to fight climate change.\\n\\nGates claimed that because he continues to “spend billions of dollars” on climate change activism, his carbon footprint isn’t an issue.\\n\\nSign up to get unfiltered news delivered straight to your inbox.\\n\\nYou can unsubscribe any time. By subscribing you agree to our Terms of Use\\n\\n“Should I stay at home and not come to Kenya and learn about farming and malaria?” Gates said in the interview with Amol Rajan.\\n\\n“I’m comfortable with the idea that not only am I not part of the problem by paying for the offsets, but also through the billions that my Breakthrough Energy Group is spending, that I’m part of the solution,” Gates added. Watch:\\n\\nEarlier this year, Gates flew around Australia on board his $70 million dollar luxury private jet lecturing people about climate change and ordering them to stop flying on planes.\\n\\nGates, who has declared that the energy crisis is a good thing, owns no fewer than FOUR private jets at a combined cost of $194 million dollars.\\n\\nA study carried out by Linnaeus University economics professor Stefan Gössling found that Gates flew more than 213,000 miles on 59 private jet flights in 2017 alone.\\n\\nGates emitted an estimated 1,760 tons of carbon dioxide emissions, over a hundred times more than the emissions per capita in the United States, according to data from the World Bank.\\n\\nElsewhere during the carefully constructed interview, Gates said he was surprised that he was targeted by ‘conspiracy theorists’ for pushing vaccines during the pandemic.\\n\\nWhile the BBC interview was set up to look like Gates was being challenged or grilled, he wasn’t asked about his close friendship with the elite pedophile Jeffrey Epstein.',\n",
    " 'lang': 'EN'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f996b2e-24be-4e8d-ac42-6aa6fc829a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7649d1c-6297-4a0b-9461-58e55d5371ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Three-Step Prompting (H3Prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96046c1-7e27-442d-b4fe-032cbd34c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../Dataset/sub_narratives_with_explanations.json', 'r') as json_file:\n",
    "    sub_narratives_with_explanations = json.load(json_file)\n",
    "with open('../Dataset/main_narratives_with_explanations.json', 'r') as json_file:\n",
    "    main_narratives_with_explanations = json.load(json_file)   \n",
    "\n",
    "# Step 1: Classify the document into a category\n",
    "def classify_category(document_text):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following document text, classify it into one of the two categories: \"Ukraine-Russia War\" or \"Climate Change\". \n",
    "\n",
    "    Document Text: \n",
    "    {document_text}\n",
    "\n",
    "    Determine the category that closely or partially fits the document. If neither category applies, return \"Other\". Return only the output, without any additional explanations or text.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Step 2: Identify the main narratives\n",
    "def classify_narratives(document_text, category):\n",
    "\n",
    "    # Generate narratives list with explanations for the given main narrative\n",
    "    narratives_list_with_explanations = \"\\n\".join(\n",
    "        f'- {narrative}: {main_narratives_with_explanations[narrative]}'\n",
    "        for narrative in narratives[category]\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    The document text given below is related to \"{category}\". \n",
    "    Please classify the document text into the most relevant narratives. Below is a list of narratives along with their explanations:\n",
    "\n",
    "    {narratives_list_with_explanations}\n",
    "\n",
    "    Document Text: \n",
    "    {document_text}\n",
    "    \n",
    "    Return the most relevant narratives as a hash-separated string (eg. Narrative1#Narrative2..). If no specific narrative can be assigned, just return \"Other\" and nothing else. Return only the output, without any additional explanations or text.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Step 3: Identify the sub-narratives based on main narratives\n",
    "def classify_sub_narrative(document_text, category, main_narrative):\n",
    "    if main_narrative == \"Hidden plots by secret schemes of powerful groups\": return \"Other\"\n",
    "    \n",
    "    # Generate sub-narratives list with explanations for the given main narrative\n",
    "    sub_narratives_list_with_explanations = \"\\n\".join(\n",
    "        f'- {sub_narrative}: {sub_narratives_with_explanations[sub_narrative]}'\n",
    "        for sub_narrative in narratives[category][main_narrative]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    The document text given below is related to \"{category}\" and its main narrative is: \"{main_narrative}\".\n",
    "    Please classify the document text into the most relevant sub-narratives. Below is a list of sub-narratives along with their explanations:\n",
    "\n",
    "    {sub_narratives_list_with_explanations}\n",
    "\n",
    "    Document Text:\n",
    "    {document_text}\n",
    "\n",
    "    Return the most relevant sub-narratives as a hash-separated string (e.g., Sub-narrative1#Sub-narrative2..). If no specific sub-narrative can be assigned, just return \"Other\" and nothing else. Return only the output, without any additional explanations or text.\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc25f7-c52b-4aa1-8c12-0fd2ee7cb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def generate_prompt(data_point):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_point: dict: Data point\n",
    "    :return: dict: tokenzed prompt\n",
    "    \"\"\"\n",
    "    output=[]\n",
    "    \n",
    "    instruction1 = classify_category(data_point['file_Content'])\n",
    "    mapping = {\"URW\": \"Ukraine-Russia War\", \"CC\":\"Climate Change\", \"NONE\": \"Other\"}\n",
    "    \n",
    "    output.append([\n",
    "            {\"role\": \"user\", \"content\": instruction1},\n",
    "            {\"role\": \"assistant\", \"content\": mapping[data_point[\"category\"]]}\n",
    "        ])\n",
    "\n",
    "    #stop if other class found\n",
    "    if mapping[data_point[\"category\"]] == \"Other\": return output\n",
    "    \n",
    "    instruction2 = classify_narratives(data_point['file_Content'], mapping[data_point[\"category\"]])\n",
    "\n",
    "    output.append([\n",
    "            {\"role\": \"user\", \"content\": instruction2},\n",
    "            {\"role\": \"assistant\", \"content\": \"#\".join(list(set(data_point[\"narratives_list\"])))}\n",
    "        ])\n",
    "\n",
    "    #stop if other class found\n",
    "    if data_point[\"narratives_list\"][0] == \"Other\": return output\n",
    "\n",
    "    m_s = defaultdict(list)\n",
    "    for m, s in zip(data_point[\"narratives_list\"], data_point['subnarratives_list']):\n",
    "        m_s[m].append(s)\n",
    "    \n",
    "    for main_narrative in list(set(data_point[\"narratives_list\"])):\n",
    "        instruction3 = classify_sub_narrative(data_point['file_Content'], mapping[data_point[\"category\"]], main_narrative)\n",
    "        \n",
    "        output.append([\n",
    "                {\"role\": \"user\", \"content\": instruction3},\n",
    "                {\"role\": \"assistant\", \"content\": \"#\".join(m_s[main_narrative])}\n",
    "            ])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80936da1-7f20-43a3-a92f-0468a7584c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(train_df_dict)):\n",
    "    train_data += generate_prompt(train_df_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed649ed-524c-42ac-8e31-502d046a21b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754857f7-99a3-4cf6-a835-edf8ca28214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict({'prompt': train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94a69d-7463-43c3-bdd2-1b2a95a5941b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples['prompt']\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1ff07-b952-45d4-b535-dd9092d154bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b0de7-c7b9-40fa-a7db-cb5d45ba2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 6, # Set this for 1 full training run.\n",
    "        # max_steps = 4000,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"lora_model\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        save_strategy = \"epoch\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f78e99-c49b-4a83-9867-09c4798fe5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b64fc-aa0b-410b-aa33-a91a971c1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9377cd8-d5a5-49ed-8847-4c3eca33b4a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c820d-1cf8-4475-ad4a-cc183af4546d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a020cd-bba9-458b-af64-8bcd598a6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f6dbd-5827-47a5-9a2e-ebdbc547ca71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:unsloth_env]",
   "language": "python",
   "name": "conda-env-unsloth_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
