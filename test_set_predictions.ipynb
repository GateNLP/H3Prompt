{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996402a9-4140-4842-8e8a-a030d5f93569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a032e88-c06b-4447-bdd2-247cb120992a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef474dc-824f-4559-9dd2-e6dd9880ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('Dataset/labels.json', 'r') as json_file:\n",
    "    narratives = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39686d24-e11b-45d1-89da-28a90817b7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef318173-d89d-495b-90ba-f657f657e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc14ec-8450-4a05-8e1f-307288c0af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = \"\"\n",
    "openai.organization = \"\" \n",
    "\n",
    "def GPT_call(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\", #gpt-3.5-turbo, gpt-4o-mini\n",
    "        messages=[\n",
    "            # {\"role\": \"system\", \"content\": \"You are a helpful AI assistant tasked with classifying text based on provided instructions and explanations. Return only the specified outputs.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775f7ae-7aaf-4817-95ea-ca7ea5c0c7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bc05d-7713-49dd-bc9d-b47d90507e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#huggingface pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d7d2d-f319-47e0-b814-ed124d226780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "# Load the model once\n",
    "llm_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map = \"auto\" # \"auto\" for mult-gpu\n",
    ")\n",
    "\n",
    "# Define the function to use the pre-loaded model\n",
    "def llm_call(prompt, pipe):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "        messages, \n",
    "        max_new_tokens=512, \n",
    "        # temperature=0.2,\n",
    "        # do_sample=True,\n",
    "        do_sample=False\n",
    "    )\n",
    "    assistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010d69c-79b9-4e4e-8cb9-feb42c275366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef35be2-4349-4184-943a-9e0027f6d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsloth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2914968-2f83-4ba4-a3cd-ec7079520dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 6000 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "        # model_name = \"training/UnslothTrain/lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "def unsloth_llm_call(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    outputs = model.generate(input_ids = inputs,\n",
    "                             max_new_tokens = 500,\n",
    "                             use_cache = True,\n",
    "                             do_sample=False,\n",
    "                             # temperature = 1.5, min_p = 0.1\n",
    "                            )\n",
    "    assistant_response = tokenizer.batch_decode(outputs)[0].split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1][:-10].strip()\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91b9c0-13fc-4a20-bd37-f23d37d07ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9a5d8-2ca9-4082-aaff-806dc817a05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1a7dc-a86a-4c39-aead-cfd6f4f93c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Three-Step Prompting (H3Prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621cf93-ec06-4ab3-8e99-9efdfa037b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('Dataset/sub_narratives_with_explanations.json', 'r') as json_file:\n",
    "    sub_narratives_with_explanations = json.load(json_file)\n",
    "with open('Dataset/main_narratives_with_explanations.json', 'r') as json_file:\n",
    "    main_narratives_with_explanations = json.load(json_file)   \n",
    "\n",
    "# Step 1: Classify the document into a category\n",
    "def classify_category(document_text):\n",
    "    prompt = f\"\"\"\n",
    "    Given the following document text, classify it into one of the two categories: \"Ukraine-Russia War\" or \"Climate Change\". \n",
    "\n",
    "    Document Text: \n",
    "    {document_text}\n",
    "\n",
    "    Determine the category that closely or partially fits the document. If neither category applies, return \"Other\". Return only the output, without any additional explanations or text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # generated_output = GPT_call(prompt)\n",
    "    # generated_output = llm_call(prompt, llm_pipe)\n",
    "    generated_output = unsloth_llm_call(prompt)\n",
    "    \n",
    "    return generated_output\n",
    "\n",
    "# Step 2: Identify the main narratives\n",
    "def classify_narratives(document_text, category):\n",
    "        \n",
    "    # Generate narratives list with explanations for the given main narrative\n",
    "    narratives_list_with_explanations = \"\\n\".join(\n",
    "        f'- {narrative}: {main_narratives_with_explanations[narrative]}'\n",
    "        for narrative in narratives[category]\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    The document text given below is related to \"{category}\". \n",
    "    Please classify the document text into the most relevant narratives. Below is a list of narratives along with their explanations:\n",
    "\n",
    "    {narratives_list_with_explanations}\n",
    "\n",
    "    Document Text: \n",
    "    {document_text}\n",
    "    \n",
    "    Return the most relevant narratives as a hash-separated string (eg. Narrative1#Narrative2..). If no specific narrative can be assigned, just return \"Other\" and nothing else. Return only the output, without any additional explanations or text.\n",
    "    \"\"\"\n",
    "\n",
    "    # generated_output = GPT_call(prompt)\n",
    "    # generated_output = llm_call(prompt, llm_pipe)\n",
    "    generated_output = unsloth_llm_call(prompt)\n",
    "\n",
    "    return generated_output\n",
    "\n",
    "# Step 3: Identify the sub-narratives based on main narratives\n",
    "def classify_sub_narrative(document_text, category, main_narrative):\n",
    "    if main_narrative == \"Hidden plots by secret schemes of powerful groups\": return \"Other\"\n",
    "    \n",
    "    # Generate sub-narratives list with explanations for the given main narrative\n",
    "    sub_narratives_list_with_explanations = \"\\n\".join(\n",
    "        f'- {sub_narrative}: {sub_narratives_with_explanations[sub_narrative]}'\n",
    "        for sub_narrative in narratives[category][main_narrative]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    The document text given below is related to \"{category}\" and its main narrative is: \"{main_narrative}\".\n",
    "    Please classify the document text into the most relevant sub-narratives. Below is a list of sub-narratives along with their explanations:\n",
    "\n",
    "    {sub_narratives_list_with_explanations}\n",
    "\n",
    "    Document Text:\n",
    "    {document_text}\n",
    "\n",
    "    Return the most relevant sub-narratives as a hash-separated string (e.g., Sub-narrative1#Sub-narrative2..). If no specific sub-narrative can be assigned, just return \"Other\" and nothing else. Return only the output, without any additional explanations or text.\n",
    "    \"\"\"\n",
    "\n",
    "    # generated_output = GPT_call(prompt)\n",
    "    # generated_output = llm_call(prompt, llm_pipe)\n",
    "    generated_output = unsloth_llm_call(prompt)\n",
    "    \n",
    "    return generated_output\n",
    "\n",
    "# Function to classify the document and return labels\n",
    "def classify_document(document_text):\n",
    "    final_label = []\n",
    "\n",
    "    # Step 1: Classify the document into a category\n",
    "    category = classify_category(document_text).replace('\"','').replace('.','').strip()\n",
    "    # print(category)\n",
    "    if category == \"Other\":\n",
    "        final_label.append(\"Other : Other\")\n",
    "    else:\n",
    "        # Step 2: Identify the main narratives\n",
    "        if category in narratives:\n",
    "            main_narratives = classify_narratives(document_text, category)\n",
    "            # print(main_narratives)\n",
    "            if main_narratives == \"Other\":\n",
    "                final_label.append(\"Other : Other\")\n",
    "            else:\n",
    "                # Step 3: Identify the single sub-narrative for each main narrative\n",
    "                main_narratives_list = main_narratives.split(\"#\")\n",
    "                main_narratives_list = [it.strip(\"-\").strip() for it in main_narratives_list]\n",
    "                all_sub_narratives = {}\n",
    "\n",
    "                for main_narrative in main_narratives_list: \n",
    "                    if main_narrative in narratives[category]:\n",
    "                        sub_narratives = classify_sub_narrative(document_text, category, main_narrative)\n",
    "                        # print(main_narrative, sub_narratives)\n",
    "                        sub_narratives_list = [sub.strip(\"-\").strip() for sub in sub_narratives.split(\"#\")]\n",
    "                        all_sub_narratives[main_narrative] = sub_narratives_list\n",
    "                    else:\n",
    "                        print(\"******* main_narrative not found *******\")\n",
    "                \n",
    "                for main, subs in all_sub_narratives.items():\n",
    "                    for sub in subs:\n",
    "                        if sub and (sub in sub_narratives_with_explanations or sub==\"Other\"):\n",
    "                            final_label.append(f\"{main} : {sub}\")\n",
    "                        else:\n",
    "                            print(\"******* sub_narrative not found *******\", sub)\n",
    "\n",
    "        else:\n",
    "            final_label.append(\"Other : Other\")\n",
    "    \n",
    "    if not final_label:\n",
    "        return [\"Other\", ['Other : Other']]\n",
    "    else:\n",
    "        return [category, final_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6240f8-f0ce-49b6-9ffe-8142a1aa47ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97234226-add9-41b2-8c5d-53c699c2c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ec0c9-5ddf-4ecd-ab0a-53ff87f55673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('Dataset/sub_narratives_with_explanations.json', 'r') as json_file:\n",
    "#     sub_narratives_with_explanations = json.load(json_file)\n",
    "# with open('Dataset/main_narratives_with_explanations.json', 'r') as json_file:\n",
    "#     main_narratives_with_explanations = json.load(json_file)\n",
    "\n",
    "# # Step 1: Identify the main narrative using binary classification\n",
    "# def classify_main_narrative(document_text, category):\n",
    "#     main_narratives = narratives[category]\n",
    "#     relevant_main_narratives = []\n",
    "    \n",
    "#     for main_narrative in main_narratives:\n",
    "#         prompt = f\"\"\"\n",
    "#         Please classify whether the document supports the following narrative:\n",
    "\n",
    "#         Narrative: \"{main_narrative}\"\n",
    "#         Explanation: {main_narratives_with_explanations[main_narrative]}\n",
    "\n",
    "#         Document Text: \n",
    "#         {document_text}\n",
    "\n",
    "#         Respond with \"Yes\" if the narrative applies to the document, or \"No\" if it does not. Return only \"Yes\" or \"No\" without any additional explanations or text.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Call the LLM to classify each main narrative\n",
    "#         # generated_output = GPT_call(prompt).strip()\n",
    "#         # generated_output = llm_call(prompt, llm_pipe).strip()\n",
    "#         generated_output = unsloth_llm_call(prompt).strip()\n",
    "        \n",
    "#         if generated_output.lower() == \"yes\":\n",
    "#             relevant_main_narratives.append(main_narrative)\n",
    "    \n",
    "#     return relevant_main_narratives\n",
    "\n",
    "# # Step 2: Identify the sub-narratives using binary classification\n",
    "# def classify_sub_narratives(document_text, category, main_narratives):\n",
    "#     all_sub_narratives = {}\n",
    "#     # print(category, main_narratives)\n",
    "#     for main_narrative in main_narratives:\n",
    "#         if main_narrative == \"Hidden plots by secret schemes of powerful groups\":\n",
    "#             continue\n",
    "        \n",
    "#         relevant_sub_narratives = []\n",
    "#         for sub_narrative in narratives[category][main_narrative]:\n",
    "#             prompt = f\"\"\"\n",
    "#             Please classify whether the document supports the following narrative:\n",
    "\n",
    "#             Narrative: \"{sub_narrative}\"\n",
    "#             Explanation: {sub_narratives_with_explanations[sub_narrative]}\n",
    "\n",
    "#             Document Text:\n",
    "#             {document_text}\n",
    "\n",
    "#             Respond with \"Yes\" if the narrative applies to the document, or \"No\" if it does not. Return only \"Yes\" or \"No\" without any additional explanations or text.\n",
    "#             \"\"\"\n",
    "            \n",
    "#             # Call the LLM for each sub-narrative classification\n",
    "#             # generated_output = GPT_call(prompt).strip()\n",
    "#             # generated_output = llm_call(prompt, llm_pipe).strip()\n",
    "#             generated_output = unsloth_llm_call(prompt).strip()\n",
    "            \n",
    "#             if generated_output.lower() == \"yes\":\n",
    "#                 relevant_sub_narratives.append(sub_narrative)\n",
    "\n",
    "#         if relevant_sub_narratives:\n",
    "#             all_sub_narratives[main_narrative] = relevant_sub_narratives\n",
    "\n",
    "#     return all_sub_narratives\n",
    "\n",
    "# # Function to classify the document and return labels\n",
    "# def classify_document(document_text):\n",
    "#     final_label = []\n",
    "\n",
    "#     # Step 1: Identify the relevant main narratives\n",
    "#     relevant_main_narratives = []\n",
    "#     for category in narratives:\n",
    "#         main_narratives = classify_main_narrative(document_text, category)\n",
    "#         relevant_main_narratives.extend(main_narratives)\n",
    "\n",
    "#     if not relevant_main_narratives:\n",
    "#         print(\"not relevant_main_narratives\")\n",
    "#         final_label.append(\"Other : Other\")\n",
    "#     else:\n",
    "#         # Step 2: Identify the sub-narratives for each main narrative\n",
    "#         all_sub_narratives = {}\n",
    "#         for main_narrative in relevant_main_narratives:\n",
    "#             category = \"Ukraine-Russia War\" if main_narrative in narratives[\"Ukraine-Russia War\"] else \"Climate Change\"\n",
    "#             sub_narratives = classify_sub_narratives(document_text, category, [main_narrative])\n",
    "#             if sub_narratives:\n",
    "#                 all_sub_narratives.update(sub_narratives)\n",
    "\n",
    "#         # Collect the final labels\n",
    "#         for main, subs in all_sub_narratives.items():\n",
    "#             for sub in subs:\n",
    "#                 if sub and sub in sub_narratives_with_explanations:\n",
    "#                     final_label.append(f\"{main} : {sub}\")\n",
    "\n",
    "#     if not final_label:\n",
    "        \n",
    "#         return [\"Other\", ['Other : Other']]\n",
    "#     else:\n",
    "#         return [category, final_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f3bd3-62f2-49e8-8aaa-18824032690b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ec1fa-f036-4d6a-b8bf-ba70acd21597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test set predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4589c-ac90-4b3b-ac78-e1140b108fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "# Directory containing the .txt files\n",
    "test_lang = \"EN\"\n",
    "directory = \"Dataset/testdata_ST12/\"+test_lang+\"/subtask-2-documents\" # download data from task website\n",
    "\n",
    "# Dictionary to store the file content\n",
    "output_file = []\n",
    "\n",
    "# Use glob to find all .txt files in the directory\n",
    "for file_path in glob.glob(f\"{directory}/*.txt\"):\n",
    "    # Extract the filename from the path\n",
    "    filename = file_path.split(\"/\")[-1]  # For Windows, use os.path.basename(file_path)\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "        category, predicted = classify_document(file_content)\n",
    "        cat_mapping = {\"Ukraine-Russia War\":\"URW\", \"Climate Change\":\"CC\", \"Other\":\"Other\"}\n",
    "        if \"climate\" in category.lower():\n",
    "            category = \"Climate Change\"\n",
    "        if \"ukraine\" in category.lower() or \"russia\" in category.lower():\n",
    "            category = \"Ukraine-Russia War\"\n",
    "        cat_code = cat_mapping[category]\n",
    "        \n",
    "        if len(predicted) == 1 and predicted[0] == \"Other : Other\":\n",
    "            narr_final_joined = \"Other\"\n",
    "            subnarr_final_joined = \"Other\"\n",
    "        else:\n",
    "            narr_final = []\n",
    "            subnarr_final = []\n",
    "            for i in range(len(predicted)):\n",
    "                narr, subnarr = predicted[i].split(\" : \")\n",
    "                narr_final.append(cat_code + \": \" + narr)\n",
    "                subnarr_final.append(cat_code + \": \" + narr + \": \" + subnarr)\n",
    "            narr_final_joined = \";\".join(narr_final)\n",
    "            subnarr_final_joined = \";\".join(subnarr_final)\n",
    "        \n",
    "        output_file.append(f\"{filename}\\t{narr_final_joined}\\t{subnarr_final_joined}\")\n",
    "        \n",
    "        print(f\"{filename}\\t{narr_final_joined}\\t{subnarr_final_joined}\")\n",
    "        \n",
    "        print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f650398-8a80-4791-b22e-9586af7c452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to the file\n",
    "with open(\"test_set_predictions/GATENLP_\"+test_lang+\"_1.txt\", \"w\") as file:\n",
    "    for line in output_file:\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9584d7-a534-49cb-8b6e-d32bf66f8bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f60d52-51ab-4347-8021-e88b04e2b7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b0b1e-214b-4c15-b671-8635ee5a8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8be43-645c-4f0c-b727-8cd1a34b2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file and saving lines into a list\n",
    "with open(\"test_set_predictions/GATENLP_\"+test_lang+\"_1.txt\", \"r\") as file:\n",
    "    lines2 = [line.split(\"\\t\")[-1].strip().split(\";\") for line in file] \n",
    "with open(\"test_set_predictions/GATENLP_\"+test_lang+\"_3.txt\", \"r\") as file:\n",
    "    lines3 = [line.split(\"\\t\")[-1].strip().split(\";\") for line in file] \n",
    "with open(\"test_set_predictions/GATENLP_\"+test_lang+\"_5.txt\", \"r\") as file:\n",
    "    lines4 = [line.split(\"\\t\")[-1].strip().split(\";\") for line in file] \n",
    "\n",
    "run_outputs = [lines2, lines3, lines4]\n",
    "\n",
    "def ensemble_union(run_outputs):\n",
    "    ensembled_results = []\n",
    "    for instance_preds in zip(*run_outputs):  # Group predictions for each instance\n",
    "        union_labels = set()\n",
    "        for run_pred in instance_preds:\n",
    "            union_labels.update(run_pred)  # Add all unique labels\n",
    "        ensembled_results.append(list(union_labels))\n",
    "    return ensembled_results\n",
    "\n",
    "# Perform union ensembling\n",
    "new_predictions = ensemble_union(run_outputs)\n",
    "\n",
    "# Directory containing the .txt files\n",
    "directory = \"Dataset/testdata_ST12/\"+test_lang+\"/subtask-2-documents\"\n",
    "output_file = []\n",
    "x=0\n",
    "for file_path in glob.glob(f\"{directory}/*.txt\"):\n",
    "    # Extract the filename from the path\n",
    "    filename = file_path.split(\"/\")[-1]  # For Windows, use os.path.basename(file_path)\n",
    "\n",
    "    if len(new_predictions[x]) == 1 and new_predictions[x][0] == \"Other\":\n",
    "        narr_final_joined = \"Other\"\n",
    "        subnarr_final_joined = \"Other\"\n",
    "    else:\n",
    "        narr_final = []\n",
    "        subnarr_final = []\n",
    "        for i in range(len(new_predictions[x])):\n",
    "            if new_predictions[x][i]==\"Other\": continue\n",
    "            items = new_predictions[x][i].split(\": \")\n",
    "            t1, t2 = \": \".join(items[:2]), new_predictions[x][i]\n",
    "            narr_final.append(t1)\n",
    "            subnarr_final.append(t2)\n",
    "        narr_final_joined = \";\".join(narr_final)\n",
    "        subnarr_final_joined = \";\".join(subnarr_final)\n",
    "    \n",
    "    x+=1\n",
    "    output_file.append(f\"{filename}\\t{narr_final_joined}\\t{subnarr_final_joined}\")\n",
    "    \n",
    "    print(f\"{filename}\\t{narr_final_joined}\\t{subnarr_final_joined}\")\n",
    "    \n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2244c-00f2-4010-a280-c5a854470f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to the file\n",
    "with open(\"test_set_predictions/GATENLP_\"+test_lang+\"_7.txt\", \"w\") as file:\n",
    "    for line in output_file:\n",
    "        file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef010b-8b74-455d-9b79-7490e53c3bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44ad55-e8b4-45a2-ba12-eb7f75edd05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1811b6a-3480-4441-8dbc-3250215b37de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2a3a2-8b2d-437b-8246-dca48aede4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:unsloth_env]",
   "language": "python",
   "name": "conda-env-unsloth_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
