{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45148a1-f61d-4514-a05c-00ddde4e3fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb250307-cd97-43e0-b106-d945a504c94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import transformers\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "prebuilt_model_id = \"lmsys/vicuna-7b-v1.5\"  # lmsys/vicuna-13b-v1.5 is also an option\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    prebuilt_model_id,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    prebuilt_model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "# Load data\n",
    "with open('../Dataset/labels.json', 'r') as json_file:\n",
    "    narratives_all = json.load(json_file)\n",
    "narratives = narratives_all['Climate Change']  # Replace with 'Ukraine-Russia War' as needed\n",
    "\n",
    "with open('../Dataset/sub_narratives_with_explanations.json', 'r') as json_file:\n",
    "    sub_narrative_contexts = json.load(json_file)\n",
    "\n",
    "# Function to parse articles from the generated text\n",
    "def parse_articles(text):\n",
    "    article_pattern = re.compile(r\"(Article \\d+:)(.*?)(?=Article \\d+:|$)\", re.DOTALL)\n",
    "    articles = article_pattern.findall(text)\n",
    "    return [content.strip().strip(\"#\") for _, content in articles]\n",
    "\n",
    "# Initialize data storage\n",
    "gen_data = []\n",
    "\n",
    "# Generate articles\n",
    "for main_narr in narratives.keys():\n",
    "    for sub_narrative in narratives[main_narr]:\n",
    "        print(f\"Processing sub-narrative: {sub_narrative}\")\n",
    "        \n",
    "        context = sub_narrative_contexts[sub_narrative]\n",
    "        articles_count = 0  # Track the number of articles generated\n",
    "        temp_values = [1.0, 1.3, 1.5]  # List of temperatures for diversity\n",
    "\n",
    "        while articles_count < 200:\n",
    "            DEFAULT_INSTRUCTIONS = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\"\"\n",
    "            USER_NAME = \"USER\"\n",
    "            BOT_NAME = \"ASSISTANT\"\n",
    "\n",
    "            # Construct the prompt\n",
    "            prompt = f\"\"\"\n",
    "            You are an AI news curator. Generate 5 different news articles related to the following topic on Climate Change.\n",
    "\n",
    "            Topic: {sub_narrative}\n",
    "            Context: {context}\n",
    "\n",
    "            Each article should be between 400-500 words and explore a unique aspect, perspective, or event related to this topic. Focus on delivering informative, coherent, and engaging articles that reflect diverse points of view or angles on the given topic. Avoid redundancy by ensuring that each article highlights a different aspect or argument related to the context provided.\n",
    "\n",
    "            The output format should look like this:\n",
    "\n",
    "            Article 1: \n",
    "            Article 2: \n",
    "            Article 3:\n",
    "            Article 4:\n",
    "            Article 5:\n",
    "            \"\"\"\n",
    "            prompt = f\"{DEFAULT_INSTRUCTIONS}\\n{USER_NAME}: {prompt}\\n{BOT_NAME}:\"\n",
    "\n",
    "            # Tokenize and generate\n",
    "            temperature = temp_values[articles_count // 20 % len(temp_values)]  # Cycle through temp_values\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids, \n",
    "                temperature=temperature, \n",
    "                max_length=30000, \n",
    "                do_sample=True\n",
    "            )\n",
    "            response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "            response = response.split(\"ASSISTANT: \")[-1]\n",
    "            articles = parse_articles(response)\n",
    "\n",
    "            # Append articles to gen_data and count them\n",
    "            for article in articles:\n",
    "                if articles_count < 200:  # Check to avoid over-generating\n",
    "                    gen_data.append({\n",
    "                        \"file_Content\": article,\n",
    "                        \"narratives_list\": [main_narr],\n",
    "                        \"subnarratives_list\": [sub_narrative]\n",
    "                    })\n",
    "                    articles_count += 1\n",
    "                else:\n",
    "                    break  # Break if we've hit 200 articles\n",
    "            \n",
    "            print(f\"Generated {articles_count}/200 articles for sub-narrative: {sub_narrative} (Temp: {temperature})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6027020-693e-47ef-a0ed-91d69090e29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee2b8d-4fb8-458f-9257-fc16be94a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Save the list of dictionaries to a JSON file\n",
    "with open('CC_gen_data.json', 'w') as json_file:\n",
    "    json.dump(gen_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a441e-8ec4-4a6c-9593-a70b1972004d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45aa28-b2e5-420b-9338-0bac71f5e5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e142986-e10a-4aa9-a79e-a0bcc44f443c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3630d-200a-4c93-97bc-46ceb20bf586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu-test] *",
   "language": "python",
   "name": "conda-env-gpu-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
